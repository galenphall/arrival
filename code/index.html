<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arrival</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\ \(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                packages: {'[+]': ['physics', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            }
        };
    </script>
    <style>
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            line-height: 1.6;
            color: #333;
            background-color: #fdfdfd;
        }
        
        .header {
            text-align: center;
            margin-bottom: 50px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 30px;
        }
        
        .title {
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #2c3e50;
        }
        
        .authors {
            font-size: 1.2em;
            color: #7f8c8d;
            margin-bottom: 5px;
        }
        
        .date {
            font-size: 1em;
            color: #95a5a6;
        }
        
        h1 {
            color: #2c3e50;
            font-size: 1.8em;
            margin-top: 40px;
            margin-bottom: 20px;
            border-bottom: 1px solid #bdc3c7;
            padding-bottom: 10px;
        }
        
        h2 {
            color: #34495e;
            font-size: 1.4em;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        h3 {
            color: #34495e;
            font-size: 1.2em;
            margin-top: 25px;
            margin-bottom: 15px;
        }
        
        .math-display {
            margin: 20px 0;
            text-align: center;
        }
        
        .note {
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .note-header {
            font-weight: bold;
            color: #2980b9;
            margin-bottom: 5px;
        }
        
        .definition {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .definition-title {
            font-weight: bold;
            color: #495057;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        ol {
            padding-left: 20px;
        }
        
        li {
            margin: 10px 0;
        }
        
        ul {
            padding-left: 20px;
        }
        
        p {
            margin: 15px 0;
            text-align: justify;
        }
        
        .equation {
            background-color: #f9f9f9;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
            overflow-x: auto;
        }
        
        .multline {
            text-align: center;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <div class="header">
        <div class="title">Arrival</div>
        <div class="authors">Polyphony & Galen</div>
        <div class="date">June 2025</div>
    </div>

    <h1>1. Setup</h1>
    
    <p>We have <em>N</em> people gathered together from different planets. Each person speaks a different language, but due to the nature of the galactic diaspora, the languages share some overlap. In particular everyone shares the same alphabet/vocabulary <strong>S</strong>. Each person <em>wants</em> access to as much information about all the others as they can get. For now, they care equally about knowing about each other person; later, that may change.</p>

    <h2>1.1 Language representation</h2>
    
    <p>All agents share a common message space <em>M</em> and symbol space <em>S</em>. Each agent <em>i</em> possesses probabilistic encoding and decoding matrices:</p>
    
    <ul>
        <li><strong>Encoder:</strong>  \(E_i \in \mathbb{R}^{|S| \times |M|}\) where \(E_i(s|m) = P_i(s|m)\) is the probability agent <em>i</em> encodes message <em>m</em> as symbol <em>s</em></li>
        <li><strong>Decoder:</strong> \(D_i \in \mathbb{R}^{|M| \times |S|}\) where \(D_i(m|s) = P_i(m|s)\) is the probability agent <em>i</em> decodes symbol <em>s</em> as message <em>m</em></li>
    </ul>

    <p>These matrices satisfy the normalization constraints:</p>
    <div class="equation">
        $$\sum_{s \in S} E_i(s|m) = 1 \quad \forall m \in M, \quad \sum_{m \in M} D_i(m|s) = 1 \quad \forall s \in S$$
    </div>

    <p>Communication between <em>i</em> and <em>j</em> proceeds through the channel:</p>
    <div class="math-display">
        $$M \xrightarrow{E_i} S \xrightarrow{D_j} M'$$
    </div>
    
    <p>where the composite channel probability is:</p>
    <div class="equation">
        $$P(m'|m; i \to j) = \sum_{s \in S} D_j(m'|s) E_i(s|m)$$
    </div>

    <h3>1.1.1 Optimal Encoder-Decoder Coupling</h3>
    
    <p>We assume that each agent's encoder is optimally adapted to their decoder to maximize self-communication. Given a decoder \(D_i\), the optimal encoder \(E_i^*\) satisfies:</p>
    <div class="equation">
        $$E_i^* = \arg\max_{E} I(M; M') \text{ subject to } P(m'|m) = \sum_{s} D_i(m'|s) E(s|m)$$
    </div>
    
    <p>where \(I(M; M')\) is the mutual information between sent and received messages when agent <em>i</em> communicates with themselves. This ensures that each agent can communicate with perfect clarity in their own language.</p>

    <h2>1.2 Dyadic information exchange</h2>
    
    <p>We assume that when <em>i</em> talks to <em>j</em> they talk long enough for <em>i</em> to cycle through the entire message and signal space (in other words we assum such communication is ergodic). But <em>i</em> may not communicate perfectly with <em>j</em>. In particular, the amount of information <em>i</em> can impart to <em>j</em> depends on the <em>mutual information</em> shared between <em>j</em>'s language and <em>i</em>'s.</p>
    
    <div class="note">
        <div class="note-header">NOTE:</div>
        Future extensions could complicate this by dropping the ergodic assumption and explicitly simulating communication.
    </div>

    <h3>1.2.1 Calculating Mutual Information</h3>
    
    <p>To calculate the mutual information \(I(M_i; M_j')\) between agent <em>i</em>'s original messages and agent <em>j</em>'s interpretations, we proceed as follows:</p>

    <ol>
        <li><strong>Define the probability distribution over messages:</strong>
            <div class="equation">
                $$P(m) = \frac{1}{|M|} \text{ for all } m \in M$$
            </div>
            This assumes uniform message usage across all agents.
        </li>
        
        <li><strong>Compute the composite channel probability:</strong>
            <div class="equation">
                $$P(m'|m; i \to j) = \sum_{s \in S} D_j(m'|s) E_i(s|m)$$
            </div>
            This captures how agent <em>i</em>'s message <em>m</em> gets interpreted by agent <em>j</em> as message <em>m'</em>.
        </li>
        
        <li><strong>Calculate the joint probability distribution:</strong>
            <div class="equation">
                $$P(m, m') = P(m) \cdot P(m'|m; i \to j) = \frac{1}{|M|} \sum_{s \in S} D_j(m'|s) E_i(s|m)$$
            </div>
        </li>
        
        <li><strong>Derive the marginal distribution for decoded messages:</strong>
            <div class="equation">
                $$P(m') = \sum_{m \in M} P(m, m') = \frac{1}{|M|} \sum_{m \in M} \sum_{s \in S} D_j(m'|s) E_i(s|m)$$
            </div>
        </li>
        
        <li><strong>Compute the mutual information:</strong>
            <div class="equation">
                $$I(M_i; M_j') = \sum_{m \in M} \sum_{m' \in M} P(m, m') \log_2 \left(\frac{P(m, m')}{P(m)P(m')}\right)$$
            </div>
            
            Given the uniform prior, this simplifies to:
            <div class="multline">
                $$\begin{align}
                I(M_i; M_j') &= \log_2 |M| + \frac{1}{|M|} \sum_{m, m'} \left(\sum_{s} D_j(m'|s) E_i(s|m)\right) \\
                &\quad \times \log_2 \left(\sum_{s} D_j(m'|s) E_i(s|m)\right) - H(M')
                \end{align}$$
            </div>
            where \(H(M') = -\sum_{m'} P(m') \log_2 P(m')\) is the entropy of the decoded message distribution.
        </li>
    </ol>

    <p>We denote the composite channel matrix as:</p>
    <div class="equation">
        $$C_{ij} = D_j E_i \in \mathbb{R}^{|M| \times |M|}$$
    </div>
    
    <p>where \(C_{ij}(m'|m) = \sum_{s \in S} D_j(m'|s) E_i(s|m)\) represents the probability that message <em>m</em> sent by agent <em>i</em> is decoded as message <em>m'</em> by agent <em>j</em>.</p>

    <p>Using this notation, the normalized mutual information (channel efficiency) is:</p>
    <div class="equation">
        $$\mathcal{I}_{ij} = \frac{I(M_i; M_j')}{H(M)} = \frac{I(M_i; M_j')}{\log_2 |M|}$$
    </div>
    
    <p>This represents the fraction of agent <em>i</em>'s information that successfully reaches agent <em>j</em>, with \(\mathcal{I}_{ij} \in [0, 1]\) where 1 indicates perfect communication.</p>

    <p>This mutual information quantifies the bits of information about agent <em>i</em>'s messages that survive the encoding-decoding process through the shared alphabet <em>S</em> and reach agent <em>j</em>.</p>

    <h2>1.3 Optimization Targets</h2>
    
    <h3>1.3.1 Information Gain</h3>
    
    <p>For the time being, we're assuming everyone just cares about getting as much information from others as possible, and doesn't care at all about whether others can understand what they say -- that's up to the others. So they are optimizing for incoming information. We also assume they care equally about information gained from every other person, so they optimize</p>
    <div class="equation">
        $$I_{i, \text{in}}(t) = \sum_{j \neq i} \mathcal{I}_{ij}$$
    </div>

    <h3>1.3.2 Information spread</h3>
    
    <p>Agents gain information by communicating with others, but information can spread both directly from \(i \rightarrow j\) and indirectly, through gossip, i.e. through some path \(i \rightarrow k \rightarrow \ldots \rightarrow j\). We assume the gossip to be truthful, or at least not intentionally deceitful. We define <em>i</em>'s <em>total</em> information gain as a weighted sum of the information gained from each <em>j</em>, where the weights are given by the column sums of the weighted Neumann matrix</p>
    <div class="equation">
        $$(\mathbb{I} - \frac{1}{N}\mathcal{I})^{-1} = \mathbb{I} + \frac{1}{N}\mathcal{I} + \frac{1}{N^2}\mathcal{I}^2 + \ldots$$,
    </div>
    which sum (for column \(j\)) the product of mutual information scores along all walks ending at node \(j\), normalized by \(N\) to ensure convergence. The goal is then to maximize this weighted combination.

    <h3>1.3.3 Home Planet Bias</h3>
    
    <p>We might assume that each person is sent as a delegate from their home planet. Over time, as a result of updating their own language, their language will drift from their home planet's, in the sense that some receiver on their planet would not be able to understand them. If we call <em>i</em>'s initial language \(L_i (t=0)\), this criterion amounts to incorporating a mutual information</p>
    <div class="equation">
        $$I_{\text{self}} = I\big(M(0); M(t)\big)$$
    </div>
    <p>into the cost function.</p>

    <h3>1.3.4 Interpretability</h3>
    
    <p>We might also assume that each person <em>does</em> care about being understood by those they talk to. Then, we would add a term to the cost function</p>
    <div class="equation">
        $$I_{i,\text{out}} = \sum_{j \neq i} \mathcal{I}_{ji}$$
    </div>

    <h2>1.4 Language update</h2>
    
    <p>Each agent can, at each time step, update their language by modifying their decoder \(D_i\).

    <h3>1.4.1 Optimal Decoder Construction</h3>
    
    <p>The decoder update follows a gradient ascent on the agent's objective function:</p>
    <div class="equation">
        $$D_i^{t+1} = \text{Normalize}\left(D_i^t + \eta \nabla_{D_i} L_i\right)$$
    </div>
    <p>where \(L_i\) is the agent's objective function (weighted combination of information gain, drift, and interpretability terms) and the normalization ensures the decoder remains a valid probability distribution. Here \(\eta\) is the learning rate which we typically set to 0.1</p>

    <h2>1.5 Optimal Encoder Construction</h2>
    
    <p>Given a decoder \(D: \mathcal{S} \to \mathcal{M}\) represented by the conditional probability matrix \(D(m|s) = P(m|s)\), we define the optimal encoder \(E: \mathcal{M} \to \mathcal{S}\) as the Bayes-optimal soft inverse of \(D\).</p>

    <div class="definition">
        <div class="definition-title">Definition: Optimal Encoder</div>
        For a given decoder \(D\) and uniform message prior \(P(m) = \frac{1}{|\mathcal{M}|}\), the optimal encoder \(E\) is defined by:
        <div class="equation">
            $$E(s|m) = \frac{D(m|s)P(s)}{\sum_{s' \in \mathcal{S}} D(m|s')P(s')}$$
        </div>
        where \(P(s) = \sum_{m \in \mathcal{M}} D(m|s)P(m) = \frac{1}{|\mathcal{M}|} \sum_{m \in \mathcal{M}} D(m|s)\) is the marginal distribution over symbols.
    </div>

    <p>This encoder maximizes the mutual information \(I(M; M')\) where \(M' = D(E(M))\), ensuring that each agent <em>i</em> can communicate optimally with themselves. That is, the composition \(D_i \circ E_i\) forms the best possible identity channel given the constraints imposed by \(D_i\).</p>

    <p>In matrix notation, if \(\mathbf{D} \in \mathbb{R}^{|\mathcal{M}| \times |\mathcal{S}|}\) and \(\mathbf{E} \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{M}|}\) represent the decoder and encoder respectively, then:</p>
    <div class="equation">
        $$\mathbf{E} = \text{diag}(\mathbf{p}_s) \mathbf{D}^T \text{diag}(\mathbf{D}^T \mathbf{p}_s)^{-1}$$
    </div>
    <p>where \(\mathbf{p}_s\) is the vector of marginal probabilities \(P(s)\) and \(\text{diag}(\cdot)\) creates a diagonal matrix from a vector.</p>

    <div class="note">
        <div class="note-header">CODE FILE:</div>
        Reproduction code for all simulations and figures can be found in <a href="arrival.ipynb" style="text-decoration: none; color: #3498db; font-weight: bold;"><code>arrival.ipynb</code></a>.
    </div>

    <h1>Results</h1>

    <div class="note">
        <div class="note-header">NOTE:</div>
        All mutual information scores presented in the following results are normalized by the entropy of the message space (i.e., divided by \(\log|M|\)) to represent the fraction of maximum possible information transfer.
    </div>

    <h2>Baseline: rapid convergence to low MI</h2>
    <div style="flex: 1; text-align: center;">
        <img src="long_simulation.png" 
             alt="10000 iterations"  
             style="max-width: 75%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
    </div>

    <p>As a general trend we find that, within the parameter region we tested, the mutual information increases rapidly and converges to a small value, typically between 0.2-0.3.</p>

    <h2>Symbol Space Size</h2>
    <div style="display: flex; gap: 20px; margin: 20px 0; justify-content: center; align-items: flex-start;">
        <div style="flex: 1; text-align: center;">
            <img src="fig1/Single_simulation_S5_M100_N5_I1000_LR0.1_HPNone_GNone.png" 
                 alt="10000 iterations" 
                 style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        </div>
        <div style="flex: 1; text-align: center;">
            <img src="fig1/Single_simulation_S10_M100_N5_I1000_LR0.1_HPNone_GNone.png" 
                 alt="10000 iterations" 
                 style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        </div>
        <div style="flex: 1; text-align: center;">
            <img src="fig1/Single_simulation_S20_M100_N5_I1000_LR0.1_HPNone_GNone.png" 
                 alt="10000 iterations" 
                 style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        </div>
        <div style="flex: 1; text-align: center;">
            <img src="fig1/Single_simulation_S50_M100_N5_I1000_LR0.1_HPNone_GNone.png" 
                 alt="10000 iterations" 
                 style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        </div>
    </div>

    <p>We test the impact of signal dimension by fixing the number of agents and the message dimension while varying \(|S|\). As can be seen in the above series of figures, in which \(|S|\) varies from \(5 \rightarrow 10 \rightarrow 20 \rightarrow 50\), the main effect of increasing the signal dimension is to slow convergence. Perhaps counterintuitively, this implies that for the interplanetary ambassadors to converge quickly on a common language they ought to constrain themselves to a small shared vocabulary.</p>

    <h2>Home Planet Preference</h2>

    <p>We next tested the effect of adding a home planet bias. As described in section 1.3.3, we implement a home planet bias as an additional term in the optimization which penalizes decoders \(D_i(t)\) as they lose mutual information with their original encoders \(E_i(0)\); in other words, it biases each agent towards remaining "fluent" in their original language. We weight this bias proportional to the rest of the gradient. The rest of the gradient in this case just comes from the mutual information between the decoder \(D_i\) and all other encoders \(E_j\).</p>

    <div style="flex: 1; text-align: center;">
        <img src="convergence_comparison.png" 
             alt="Comparison simulation with N=5, I=1000" 
             style="max-width: 75%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
    </div>
    
    <p>As expected, an increasingly steep Home Planet bias decreases the rate of language convergence. However, we note that weighting the home planet at 20% (so, equal to the weight of the MI with each of the four other agents) does not make a significant difference to the convergence rate, indicating some potential for navigating this tradeoff in our model.</p>

    <p>Due to time constraints, we could not implement the other possible extensions described above.</p>

    <h1>Questions, Future Directions, and Applications</h1>

    <h2>What incentive and/or network structures produce assimilation versus preservation?</h2>

    <p>Given competing pressures to maximize intelligibility of alien languages while minimizing drift from one's home language, under what constraints do agents converge on a new language (lose their home language) vs retain their home language? We show that the former occurs when interpretability of a member of one's home planet is weighted equally to interpretability of alien agents. However, increasing the bias towards one's home language should delay convergence. More interesting dynamics may emerge when agents are given heterogeneous goals. We predict that if aliens from planet A have a larger bias to retain their home language relative to aliens from planet B, then the planet B population should assimilate to the language from planet A. However, it is unclear how many agents with such a bias are needed to produce this sort of percolation effect. For example, if one agent maximally retains their home language, do all other agents converge around that individual, or is a critical population size (or distribution of interactions) required? There is a similar question to be asked concerning the emergence of cliques of language users: Agents may bias interpreting certain agents over others, or exhibit a bias towards preferentially interacting with more interpretable agents, leading to convergence on a set of communication schemes. By exploring its parameter space, this model can be used to understand the competing drives of assimilation versus preservation on communication schemes when differing populations come into contact. This tension is seen it at least three areas:</p>

        <ol>
            <li><strong>The migration and contact of different human populations:</strong> How do languages form or adapt when two populations come into contact? How do these dynamics change when the populations are equally represented versus when one population is a minority within a dominant culture?</li>
            
            <li><strong>The communication dynamics of interdisciplinary scientific research:</strong> Where a scientist may want to gain insights from other disciplines but not lose the ability to meaningfully integrate them with their home discipline.</li>
            
            <li><strong>The communication dynamics observed in cross-cultural research:</strong> For example, an anthropologist may want to understand the cultural practices, language, and worldview of a novel population but must also be able to convey this information intelligibly back to their field.</li>
        </ol>

    <h2>What are optimal deceit strategies?</h2>

    <p>In this model, agents are not incentivized to make their language interpretable to agents from other planets. They are "greedy information gatherers." An additional incentive be interpretable would likely further encourage language convergence (similar to RSA models of convention formation / language evolution, see Hawkins et al., 2023). This approach would model collaborative interactions. However, what is perhaps less well understood is what group-level patterns emerge if agents exhibit a drive to <em>minimize</em> how well they are interpretable by other agents -- that is, to both gather as much information from other agents while obscuring their own information from other agents as much as possible. This incentive structure might be likened to a game of poker, where players must gather as much information about other players' cards while revealing as little as possible about their own hand. Work by Frey et al. (2018) shows that in a game of No-Limit Texas Hold-em, successful players are distinguished from unsuccessful players not simply by how much information they process but how they process it. Namely, successful players integrate information about other players' hands (public) with information about their own hand (private) to produce new information that is not reducible to the mere combination of the public and private information. As a result, other players cannot easily infer ("reverse engineer") the contents of a successful player's hand merely by observing their reaction to publicly available information. In the model presented above, introducing an incentive to maximize intelligibility of other agents while minimizing one's own intelligibility <em>to</em> other agents may provide a minimal model for simulating these empirical findings and examining information processing strategies in deceitful contexts.</p>

    <h2>What is the impact of energy constraints?</h2>

    <p>Currently, there is not a constraint on the complexity of the communication scheme an agent may develop in order to maximally interpret other agents (including their own home planet). One might predict that optimizing for maximal interpretability of other communication schemes might produce increasingly complex languages. However, the complexity of human languages is limited by energy constraints (e.g., Tishby et al., 2000; Ferrer i Cancho & Solé, 2003; Piantadosi et al., 2012). We believe that examining how the complexity of communication schemes changes in this model over time, and how an additional bias against complexity may alter the model's dynamics, offers an interesting future direction of exploration. A tradeoff between maximal interpretability of other communication schemes and minimal communicative complexity may allow for a basic model of how energy constraints govern communication dynamics as disciplines evolve, interact, and merge. For example, a scientific discipline may develop increasingly complex communication schemes to accommodate increases in knowledge within the domain, leading to instability that may cause the scheme to transition into a new state that represents knowledge in a more parsimonious way -- a paradigm shift.</p>

    <h1>References</h1>
    
    <p>Ferrer i Cancho, R., & Solé, R. V. (2003). Least effort and the origins of scaling in human language. <em>Proceedings of the National Academy of Sciences</em>, <em>100</em>(3), 788-791.</p>
    
    <p>Frey, S., Albino, D. K., & Williams, P. L. (2018). Synergistic information processing encrypts strategic reasoning in poker. <em>Cognitive Science</em>, <em>42</em>(5), 1457-1476.</p>
    
    <p>Hawkins, R. D., Franke, M., Frank, M. C., Goldberg, A. E., Smith, K., Griffiths, T. L., & Goodman, N. D. (2023). From partners to populations: A hierarchical Bayesian account of coordination and convention. <em>Psychological Review</em>, <em>130</em>(4), 977.</p>
    
    <p>Piantadosi, S. T., Tily, H., & Gibson, E. (2012). The communicative function of ambiguity in language. <em>Cognition</em>, <em>122</em>(3), 280-291.</p>
    
    <p>Tishby, N., Pereira, F. C., & Bialek, W. (2000). The information bottleneck method. <em>arXiv preprint physics/0004057</em>.</p>

</body>
</html>