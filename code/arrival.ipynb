{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc76e71-ccc1-46c3-af12-17c104c94008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import wraps\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def gen_D_binary(S, M, key=None):\n",
    "    \"\"\"Generate a decoder matrix D where D[m,s] = P(m|s)\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "    \n",
    "    D = jnp.zeros((M, S))\n",
    "    for col in range(S):\n",
    "        subkey1, subkey2, key = jax.random.split(key, 3)\n",
    "        # For each symbol, randomly assign probabilities to messages\n",
    "        num_messages = jax.random.randint(subkey1, (), 1, M+1)\n",
    "        selected_messages = jax.random.choice(subkey2, M, shape=(M,), replace=True)\n",
    "        unique_messages = jnp.unique(selected_messages[:num_messages])\n",
    "        D = D.at[unique_messages, col].set(1.0 / len(unique_messages))\n",
    "    return D\n",
    "\n",
    "def gen_optimal_encoder(D):\n",
    "    \"\"\"\n",
    "    Generate the optimal encoder E given decoder D.\n",
    "    D: (M, S) matrix where D[m,s] = P(m|s)\n",
    "    Returns E: (S, M) matrix where E[s,m] = P(s|m)\n",
    "    \"\"\"\n",
    "    M, S = D.shape\n",
    "    \n",
    "    # Compute marginal P(s) under uniform P(m) = 1/M\n",
    "    P_m = 1.0 / M\n",
    "    P_s = jnp.sum(D * P_m, axis=0)  # P(s) = sum_m P(m|s)P(m)\n",
    "    \n",
    "    # Compute encoder using Bayes rule\n",
    "    # E(s|m) = D(m|s)P(s) / sum_s' D(m|s')P(s')\n",
    "    # Vectorized computation\n",
    "    numerator = D.T * P_s[:, jnp.newaxis]  # (S, M)\n",
    "    denominator = jnp.sum(D * P_s[jnp.newaxis, :], axis=1)  # (M,)\n",
    "    E = numerator / (denominator[jnp.newaxis, :])\n",
    "    \n",
    "    return E\n",
    "\n",
    "@jit\n",
    "def calculate_mutual_information_jax(E_i, D_j, P_m=None):\n",
    "    \"\"\"\n",
    "    JAX-compatible calculation of normalized mutual information.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    E_i : jnp.ndarray\n",
    "        Encoder matrix for agent i, shape (|S|, |M|) where E_i[s,m] = P(s|m)\n",
    "    D_j : jnp.ndarray  \n",
    "        Decoder matrix for agent j, shape (|M|, |S|) where D_j[m',s] = P(m'|s)\n",
    "    P_m : jnp.ndarray, optional\n",
    "        Prior distribution over messages, shape (|M|,). If None, assumes uniform.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    I_ij_normalized : float\n",
    "        Normalized mutual information I(M_i; M_j')/H(M), in range [0,1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get dimensions\n",
    "    num_symbols, num_messages = E_i.shape\n",
    "    \n",
    "    # Set uniform prior if not provided\n",
    "    if P_m is None:\n",
    "        P_m = jnp.ones(num_messages) / num_messages\n",
    "    \n",
    "    # Compute composite channel matrix C_ij\n",
    "    C_ij = D_j @ E_i\n",
    "    \n",
    "    # Compute joint probability matrix\n",
    "    P_joint = C_ij * P_m[jnp.newaxis, :]\n",
    "    \n",
    "    # Compute marginal P(m')\n",
    "    P_m_prime = jnp.sum(P_joint, axis=1)\n",
    "    \n",
    "    # Compute mutual information using vectorized operations\n",
    "    epsilon = 1e-10\n",
    "    # Create outer product of marginals\n",
    "    P_marginal_product = jnp.outer(P_m_prime, P_m)\n",
    "\n",
    "    # TODO: is this logic actually robust to p=0?\n",
    "    # Compute MI with numerical stability\n",
    "    # Using xlogy for x * log(y) which handles x=0 case properly\n",
    "    log_ratio = P_joint / (P_marginal_product + epsilon)\n",
    "    mutual_info = jnp.sum(jax.scipy.special.xlogy(P_joint, log_ratio))\n",
    "    \n",
    "    # Normalize by entropy of M\n",
    "    H_M = -jnp.sum(P_m * jnp.log(P_m))\n",
    "    I_ij_normalized = mutual_info / H_M\n",
    "    \n",
    "    return I_ij_normalized\n",
    "\n",
    "def construct_matrix_G(I, regularization=1e-6):\n",
    "    \"\"\"\n",
    "    Construct matrix G from matrix I by solving the system:\n",
    "    G = (I - MI/N)^(-1)\n",
    "    \n",
    "    Uses CPU computation to avoid Metal/GPU compatibility issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    I : numpy.ndarray\n",
    "        Input matrix I (mutual information matrix)\n",
    "    regularization : float\n",
    "        Small value added to diagonal for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        The constructed matrix G\n",
    "    \"\"\"\n",
    "    # Convert to numpy and use CPU computation to avoid Metal backend issues\n",
    "    I_np = np.array(I) / N\n",
    "    n = I_np.shape[0]\n",
    "    identity = np.eye(n)\n",
    "    \n",
    "    # Add small regularization to ensure matrix is invertible\n",
    "    matrix_to_invert = identity - I_np + regularization * identity\n",
    "    \n",
    "    try:\n",
    "        # Use numpy for CPU-only computation\n",
    "        G_np = np.linalg.solve(matrix_to_invert, identity)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Fallback: if solve fails, use pseudoinverse\n",
    "        try:\n",
    "            G_np = np.linalg.pinv(matrix_to_invert) @ identity\n",
    "        except:\n",
    "            # Ultimate fallback: return identity matrix\n",
    "            G_np = identity\n",
    "    \n",
    "    # Convert back to JAX array\n",
    "    return jnp.array(G_np)\n",
    "\n",
    "@jit\n",
    "def mutual_info_sum(D, E_arr, v=None):\n",
    "    \"\"\"\n",
    "    Calculate weighted sum of mutual information for decoder D with multiple encoders.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    D : jnp.ndarray\n",
    "        Decoder matrix, shape (|M|, |S|)\n",
    "    E_arr : list of jnp.ndarray\n",
    "        List of encoder matrices, each shape (|S|, |M|)\n",
    "    v : jnp.ndarray, optional\n",
    "        Weights for each encoder, default is uniform\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weighted_sum : float\n",
    "        Weighted sum of mutual information values\n",
    "    \"\"\"\n",
    "    n = len(E_arr)\n",
    "    if v is None:\n",
    "        v = jnp.ones(n) / n\n",
    "    \n",
    "    # Stack encoders for vectorized computation\n",
    "    E_stack = jnp.stack(E_arr, axis=0)  # (n, |S|, |M|)\n",
    "    \n",
    "    # Vectorized mutual information calculation\n",
    "    vmap_mi = vmap(lambda E: calculate_mutual_information_jax(E, D))\n",
    "    mi_values = vmap_mi(E_stack)\n",
    "    \n",
    "    return jnp.sum(v * mi_values)\n",
    "\n",
    "@jit\n",
    "def info_grad(D, E_arr, v=None):\n",
    "    \"\"\"\n",
    "    Compute gradient of mutual information sum with respect to decoder D.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    D : jnp.ndarray  \n",
    "        Decoder matrix for agent j, shape (|M|, |S|) where D_j[m',s] = P(m'|s)\n",
    "    E_arr : list[jnp.ndarray] of length n\n",
    "        List of encoder matrices for agent i, shape (|S|, |M|) where E_i[s,m] = P(s|m)\n",
    "    v : jnp.ndarray\n",
    "        A n-vector of weights to prioritize the mutual information from different channels\n",
    "        default = uniform weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    grad_D: jnp.ndarray\n",
    "        The gradient of the weighted mutual information sum with respect to D\n",
    "    \"\"\"\n",
    "    # Create gradient function\n",
    "    grad_fn = grad(mutual_info_sum, argnums=0)\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad_D = grad_fn(D, E_arr, v)\n",
    "    \n",
    "    return grad_D\n",
    "\n",
    "def normalize_decoder(D):\n",
    "    \"\"\"\n",
    "    Normalize decoder matrix so columns sum to 1.\n",
    "    \"\"\"\n",
    "    return (D + 1e-10) / (jnp.sum(D + 1e-10, axis=0, keepdims=True))\n",
    "\n",
    "def update(D, E_arr, learning_rate=0.01, v=None):\n",
    "    \"\"\"\n",
    "    Update decoder D to maximize weighted mutual information from encoders E_arr.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    D : jnp.ndarray\n",
    "        Current decoder matrix\n",
    "    E_arr : list[jnp.ndarray]\n",
    "        List of encoder matrices to receive from\n",
    "    learning_rate : float\n",
    "        Step size for gradient ascent\n",
    "    v : jnp.ndarray, optional\n",
    "        Weights for each encoder\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    D_new : jnp.ndarray\n",
    "        Updated and normalized decoder matrix\n",
    "    \"\"\"\n",
    "    # Compute gradient\n",
    "    grad_D = info_grad(D, E_arr, v)\n",
    "    \n",
    "    # Gradient ascent step\n",
    "    D_new = D + learning_rate * grad_D\n",
    "\n",
    "    # Set any negative values to 0\n",
    "    D_new = jnp.maximum(D_new, 0.0)\n",
    "    \n",
    "    # Project back to probability simplex by normalization\n",
    "    D_new = normalize_decoder(D_new)\n",
    "    \n",
    "    return D_new\n",
    "\n",
    "# Additional utilities for the full system\n",
    "\n",
    "def initialize_agents(N, S, M, key=None):\n",
    "    \"\"\"\n",
    "    Initialize N agents with random decoders and optimal encoders.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    N : int\n",
    "        Number of agents\n",
    "    S : int\n",
    "        Size of shared symbol space\n",
    "    M : int\n",
    "        Size of message space\n",
    "    key : jax.random.PRNGKey\n",
    "        Random key for initialization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    decoders : list of jnp.ndarray\n",
    "        List of decoder matrices\n",
    "    encoders : list of jnp.ndarray\n",
    "        List of optimal encoder matrices\n",
    "    \"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(0)\n",
    "    \n",
    "    decoders = []\n",
    "    encoders = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # Initialize random decoder\n",
    "        D = jax.random.uniform(subkey, (M, S))\n",
    "        D = normalize_decoder(D)\n",
    "        decoders.append(D)\n",
    "        \n",
    "        # Compute optimal encoder\n",
    "        E = gen_optimal_encoder(D)\n",
    "        encoders.append(E)\n",
    "    \n",
    "    return decoders, encoders\n",
    "\n",
    "def simulate_convergence(\n",
    "    N, S, M, num_iterations=100, learning_rate=0.01, key=None, \n",
    "    home_planet=None, gossip=None):\n",
    "    \"\"\"\n",
    "    Simulate the convergence of the multi-agent language system.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    N : int\n",
    "        Number of agents\n",
    "    S : int\n",
    "        Size of shared symbol space\n",
    "    M : int\n",
    "        Size of message space\n",
    "    num_iterations : int\n",
    "        Number of update iterations\n",
    "    learning_rate : float\n",
    "        Learning rate for decoder updates\n",
    "    key : jax.random.PRNGKey\n",
    "        Random key for initialization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : dict\n",
    "        Dictionary containing evolution history\n",
    "    \"\"\"\n",
    "    # Initialize agents\n",
    "    decoders, encoders = initialize_agents(N, S, M, key)\n",
    "\n",
    "    # Record mutual information matrix\n",
    "    mi_matrix = jnp.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j:\n",
    "                mi_matrix = mi_matrix.at[i, j].set(\n",
    "                    calculate_mutual_information_jax(encoders[j], decoders[i])\n",
    "                )\n",
    "    \n",
    "    # Track history\n",
    "    history = {\n",
    "        'mutual_info': [mi_matrix],\n",
    "        'decoders': [decoders],\n",
    "        'encoders': [encoders]\n",
    "    }\n",
    "    \n",
    "    for iteration in tqdm(range(num_iterations)):\n",
    "        # Update each agent's decoder\n",
    "        new_decoders = []\n",
    "        new_encoders = []\n",
    "\n",
    "        if gossip is not None:\n",
    "            G = construct_matrix_G(mi_matrix)\n",
    "        \n",
    "        for i in range(N):\n",
    "            # Get encoders from all other agents\n",
    "            other_encoders = [encoders[j] for j in range(N) if j != i]\n",
    "\n",
    "            if home_planet is not None:\n",
    "                other_encoders.append(history['encoders'][0][i]) # Also optimize for mutual info with original language\n",
    "\n",
    "            v = np.ones(len(other_encoders))\n",
    "            \n",
    "            if isinstance(home_planet, float) or isinstance(home_planet, int):\n",
    "                v[-1] = home_planet # Weight according to passed-in value\n",
    "\n",
    "            if gossip is not None:\n",
    "                v[:N-1] = gossip * G[:, jnp.arange(N) != i].sum(0)\n",
    "\n",
    "            v /= v.sum()\n",
    "            \n",
    "            # Update decoder\n",
    "            D_new = update(decoders[i], other_encoders, learning_rate, v=v)\n",
    "            new_decoders.append(D_new)\n",
    "            \n",
    "            # Compute new optimal encoder\n",
    "            E_new = gen_optimal_encoder(D_new)\n",
    "            new_encoders.append(E_new)\n",
    "        \n",
    "        decoders = new_decoders\n",
    "        encoders = new_encoders\n",
    "        \n",
    "        # Record mutual information matrix\n",
    "        mi_matrix = jnp.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    mi_matrix = mi_matrix.at[i, j].set(\n",
    "                        calculate_mutual_information_jax(encoders[j], decoders[i])\n",
    "                    )\n",
    "        \n",
    "        history['mutual_info'].append(mi_matrix)\n",
    "        history['decoders'].append(decoders)\n",
    "        history['encoders'].append(encoders)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage and testing\n",
    "# Set parameters\n",
    "N = 10 # Number of agents\n",
    "S = 100 # Symbol space size\n",
    "M = 100 # Message space size\n",
    "home_planet = None\n",
    "gossip = None\n",
    "learning_rate=0.1\n",
    "iterations = 1000\n",
    "\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(42)\n",
    "decoders, encoders = initialize_agents(N, S, M, key)\n",
    "\n",
    "# Test gradient computation\n",
    "D_test = decoders[0]\n",
    "E_test = encoders[1:]\n",
    "\n",
    "# Compute gradient\n",
    "grad_D = info_grad(D_test, E_test)\n",
    "print(f\"Gradient shape: {grad_D.shape}\")\n",
    "print(f\"Gradient norm: {jnp.linalg.norm(grad_D):.4f}\")\n",
    "\n",
    "# Test update\n",
    "D_new = update(D_test, E_test, learning_rate=learning_rate)\n",
    "print(f\"Decoder still normalized: {jnp.allclose(jnp.sum(D_new, axis=0), 1.0)}\")\n",
    "\n",
    "# Run short simulation\n",
    "history = simulate_convergence(N, S, M, num_iterations=iterations, learning_rate=learning_rate, key=key, \n",
    "    home_planet=home_planet, gossip=1)\n",
    "\n",
    "print(f\"\\nInitial average mutual information: {jnp.mean(history['mutual_info'][0]):.4f}\")\n",
    "print(f\"Final average mutual information: {jnp.mean(history['mutual_info'][-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results from the single simulation\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Calculate mean MI and percentiles for each iteration\n",
    "mean_mi = []\n",
    "\n",
    "for mi_matrix in history['mutual_info']:\n",
    "    # Get non-zero, non-diagonal elements\n",
    "    \n",
    "    mean_mi.append(jnp.mean(mi_matrix[mi_matrix > 0]))\n",
    "\n",
    "iterations_range = range(len(mean_mi))\n",
    "\n",
    "# Plot mean line\n",
    "plt.plot(iterations_range, mean_mi, 'blue', linewidth=2, label='Mean Mutual Information')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mutual Information')\n",
    "plt.title(f'Convergence of Multi-Agent Language System\\nN={N}, S={S}, M={M}, Learning Rate={learning_rate}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"Single_simulation_S{S}_M{M}_N{N}_I{iterations}_LR{learning_rate}_HP{home_planet}_G{gossip}.png\", \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Initial mean MI: {mean_mi[0]:.4f}\")\n",
    "print(f\"Final mean MI: {mean_mi[-1]:.4f}\")\n",
    "print(f\"Total improvement: {mean_mi[-1] - mean_mi[0]:.4f}\")\n",
    "print(f\"Percentage improvement: {(mean_mi[-1] - mean_mi[0])/mean_mi[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afee02-7606-4dcd-9ae5-da1fa787bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and testing\n",
    "# Set parameters\n",
    "N = 10 # Number of agents\n",
    "S = 100 # Symbol space size\n",
    "M = 100 # Message space size\n",
    "home_planet = 1\n",
    "gossip = 0\n",
    "learning_rate=0.05\n",
    "iterations = 100\n",
    "\n",
    "# Define different home planet bias values to test\n",
    "home_planet_values = [0, 1, 10, 100, 1000]\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "# Store results for each simulation\n",
    "all_results = {}\n",
    "\n",
    "# Run simulations for each home planet value\n",
    "for hp_value in home_planet_values:\n",
    "    print(f\"\\nRunning simulation with home_planet = {hp_value}\")\n",
    "    \n",
    "    # Reset random key for consistency\n",
    "    sim_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # Run simulation\n",
    "    history_hp = simulate_convergence(N, S, M, num_iterations=iterations, \n",
    "                                     learning_rate=learning_rate, key=sim_key, \n",
    "                                     home_planet=hp_value if hp_value > 0 else None)\n",
    "    \n",
    "    # Calculate mean MI and percentiles for each iteration\n",
    "    mean_mi_hp = []\n",
    "    p25_hp = []\n",
    "    p75_hp = []\n",
    "    \n",
    "    for mi_matrix in history_hp['mutual_info']:\n",
    "        # Get non-zero, non-diagonal elements\n",
    "        mask = (mi_matrix > 0) & (mi_matrix != jnp.diag(jnp.diag(mi_matrix)))\n",
    "        valid_mi = mi_matrix[mask]\n",
    "        \n",
    "        mean_mi_hp.append(jnp.mean(valid_mi))\n",
    "        p25_hp.append(jnp.percentile(valid_mi, 25))\n",
    "        p75_hp.append(jnp.percentile(valid_mi, 75))\n",
    "    \n",
    "    all_results[hp_value] = {\n",
    "        'mean_mi': mean_mi_hp,\n",
    "        'p25': p25_hp,\n",
    "        'p75': p75_hp,\n",
    "        'history': history_hp\n",
    "    }\n",
    "    \n",
    "    print(f\"Final mean MI: {mean_mi_hp[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Create the comparison plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for i, hp_value in enumerate(home_planet_values):\n",
    "    mean_mi_data = all_results[hp_value]['mean_mi']\n",
    "    p25_data = all_results[hp_value]['p25']\n",
    "    p75_data = all_results[hp_value]['p75']\n",
    "\n",
    "    hp_pct = int(hp_value / (N - 1 + hp_value) * 100)\n",
    "    \n",
    "    label = f'Home Planet Bias = {hp_pct}%' if hp_value > 0 else 'No Home Planet'\n",
    "    iterations_range = range(len(mean_mi_data))\n",
    "    \n",
    "    # Plot mean line\n",
    "    plt.plot(iterations_range, mean_mi_data, \n",
    "             color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    # Add shaded region for 25th-75th percentiles\n",
    "    plt.fill_between(iterations_range, p25_data, p75_data, \n",
    "                     color=colors[i], alpha=0.2)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Mutual Information')\n",
    "plt.title('Convergence Comparison: Effect of Home Planet Bias\\n(Shaded regions show 25th-75th percentiles)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"Convergence_comparison_S{S}_M{M}_N{N}_I{iterations}_LR{learning_rate}.png\", \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "for hp_value in home_planet_values:\n",
    "    mean_mi_data = all_results[hp_value]['mean_mi']\n",
    "    initial_mi = mean_mi_data[0]\n",
    "    final_mi = mean_mi_data[-1]\n",
    "    improvement = final_mi - initial_mi\n",
    "    \n",
    "    label = f\"HP={hp_value}\" if hp_value > 0 else \"No HP\"\n",
    "    print(f\"{label:8s}: Initial={initial_mi:.4f}, Final={final_mi:.4f}, \"\n",
    "          f\"Improvement={improvement:.4f} ({improvement/initial_mi*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
